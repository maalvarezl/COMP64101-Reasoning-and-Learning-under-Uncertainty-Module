{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc7e383-bb3d-4d22-b342-39d220baf415",
   "metadata": {
    "id": "fbc7e383-bb3d-4d22-b342-39d220baf415",
    "tags": []
   },
   "source": [
    "# COMP64101 - Inference Lab\n",
    "\n",
    "### Goals of This Weeks lab:\n",
    "1. Understand Monte Carlo Estimation\n",
    "2. Understand Markov Chain Monte Carlo Methods\n",
    "\n",
    "\n",
    "### <font color='289C4E'>Table of contents<font><a class='anchor' id='top'></a>\n",
    "- [Learning Human Game Strategies](#0.5)\n",
    "- [1. Monte Carlo Approximation](#1)\n",
    "- [2. An Introduction to Markov Chain Monte Carlo (MCMC) ](#2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa722eb-ae41-495d-b41a-19663c3789ab",
   "metadata": {
    "id": "9aa722eb-ae41-495d-b41a-19663c3789ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell first!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_human_utility():\n",
    "    xs = np.arange(0.5,60,0.5)\n",
    "    ys = np.log(xs)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xs,ys)\n",
    "    plt.title(\"A plot of Human Utility for Different Levels of Income\")\n",
    "    plt.xlabel(\"Income (£1000's)\")\n",
    "    plt.ylabel(\"Associated Utility\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mcs_sample_human_utility():\n",
    "    mean = 3\n",
    "    sd = 1\n",
    "    x = np.random.normal(3,0.1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ae6cc-5169-46f6-977f-1e7654c67eff",
   "metadata": {
    "id": "5c3ae6cc-5169-46f6-977f-1e7654c67eff",
    "tags": []
   },
   "source": [
    "## Learning Human Game Strategies <a class=\"anchor\" id=\"0.5\"></a>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://static1.cbrimages.com/wordpress/wp-content/uploads/2023/03/best-selling-action-adventure-games-of-all-time-featured-image.jpg?q=50&fit=crop&w=1100&h=618&dpr=1.5\" width=1000>\n",
    "</center>\n",
    "\n",
    "\n",
    "<h3>What does it mean to learn human goals?</h3>\n",
    "<p style=\"font-size:15px\">\n",
    "While humans often perform predictable actions, such as eating when they are hungry or sleeping when they are tired, human behaviour is often very hard to predict. The main reason for this is that the human brain is able to optimize over huge numbers of potential action/reward spaces.\n",
    "    \n",
    "Consider the goals of a person who is trying to win a combat game such as Call of Duty Online. It is extremely difficult to write these factors into an equation which can model their behaviour.\n",
    "\n",
    "Can you think of an equation which might accurately capture this behaviour?\n",
    "\n",
    "Did you remember to include:\n",
    "- Risk of being behind enemy lines\n",
    "- Chance of encountering enemies\n",
    "- Number of hiding spots in the region\n",
    "\n",
    "How about things like:\n",
    "- Distractions from phones\n",
    "- Needing to eat\n",
    "- Tiredness\n",
    "\n",
    "If you sit for a few hours, you may be able to come up with many factors which determine human behaviour. However, I am very confident that you won't get them all! I am also confident that you will strugle to write these into an equation. Researchers from many fields are trying to better understand human behaviour. Throughout this notebook, we will use MCMC techniques along with some simplistic utility functions to try to predict human goals.  \n",
    "</p>\n",
    "\n",
    "\n",
    "<h3>A simplistic Model for Human Behaviour!</h3>\n",
    "\n",
    "<h4>Utility functions</h4>\n",
    "\n",
    "In economics and decision theory, a utility function is used to assign preferences over a set or actions of outcomes. It is common to see plots such as the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd0423-e7d8-4b11-8c8b-9e600762c2d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "85dd0423-e7d8-4b11-8c8b-9e600762c2d4",
    "outputId": "7be4d09c-f849-4240-935e-2d924f39aee2",
    "tags": [
     "test_tag"
    ]
   },
   "outputs": [],
   "source": [
    "plot_human_utility()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2991706a-c378-44d6-9cf3-a41faf596462",
   "metadata": {
    "id": "2991706a-c378-44d6-9cf3-a41faf596462",
    "tags": [
     "test_tag"
    ]
   },
   "source": [
    "<script>\n",
    "  function searchCellsByTag(tag) {\n",
    "    // Get the notebook's cell list\n",
    "    var cells = Jupyter.notebook.get_cells();\n",
    "      \n",
    "    // Iterate through each cell\n",
    "    cells.forEach(function(cell, index) {\n",
    "      // Check if the cell has metadata tags\n",
    "      if (cell.metadata.tags && cell.metadata.tags.includes(tag)) {\n",
    "        console.log('Cell', index, 'has the tag:', tag);\n",
    "        // You can perform additional actions here, such as highlighting the cell\n",
    "        cell.element.css('border', '2px solid red');\n",
    "      }\n",
    "    });\n",
    "  }\n",
    "\n",
    "  // Example usage: search for cells with the tag 'my_tag'\n",
    "  searchCellsByTag('test_tag');\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92459d3-d910-4487-837b-a3b24088abe4",
   "metadata": {
    "id": "b92459d3-d910-4487-837b-a3b24088abe4"
   },
   "source": [
    "This artificial plot implies that initially small increases in income are desireable for the person but eventually this will level out and much larger income shifts are needed to get the same level of desirability. These types of plots can easily be adapted to AI settings, where the goals of a human need to be modelled. This is common in AI assistance problems for example, where an AI assistant tries to predict the goals of a human and recommends actions to take.\n",
    "\n",
    "<h4>A simplistic Approximation to Utility of Gamers</h4>\n",
    "\n",
    "As previously stated, it is difficult to fully model a human's goals. However, we can begin to make simple approximations by considering the goals with highest priority.\n",
    "\n",
    "Let us consider the utility of someone who is playing Call of Duty. The player will probably have some willingness to use ammunition (the utility function itself) and an estimated amount of ammo to beat the opponent (the parameter of the utility function). We model this with a function $f_\\omega^{(1)} (s)$\n",
    "\n",
    "$$ f_\\omega^{(1)} (s_i) = 1 - \\Phi(s_i)$$\n",
    "\n",
    "where $\\Phi(d)$ the cdf of a normal distribution $N(\\mu, \\sigma)$ at x=$d$ and $s$ is a vector of parameters representing the scenario. For example, $s_1$ represents the predicted amount of ammo needed to beat the opponent.\n",
    "\n",
    "The gamer will also consider the probability of success against their opponent. This can be modelled as follows:\n",
    "$$ f_\\omega^{(2)} (s) = p(\\text{success} | s_2)$$\n",
    "\n",
    "which represents the probability of beating the opponent given their level $s_2$.\n",
    "\n",
    "We can combine this into a utility function as follows:\n",
    "\n",
    "$$ U(s) = f_\\omega^{(1)} (s) f_\\omega^{(2)}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2cba8-f4e3-4de7-892f-5d2df0952897",
   "metadata": {
    "id": "d0d2cba8-f4e3-4de7-892f-5d2df0952897"
   },
   "source": [
    "## 1.  Monte Carlo Approximation<a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "Monte Carlo approximation is a technique which is used to estimate statistical results through random sampling. The algorithm simply draws random samples from the distribution and looks at the statistics of those specific samples. For example, the expected value can be written as follows:\n",
    "\n",
    "$$ \\mathbb{E}[X] \\approx \\frac{1}{N} \\sum_{i=1}^{N} x_i$$\n",
    "\n",
    "The key to this approximation method is that we must be able to draw samples from the distribution. Lets say that the gamer's willingness to use ammo can be modelled by a $N(\\mu, 0.1)$ distribution but we don't know the value of $\\mu$. In an AI setting, we could sample the distribution by observing the gamers behaviour or directly recommending actions.\n",
    "\n",
    "Below is a plot which shows how the predicted mean varies upon taking more samples from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d41f01-cc9d-44ed-b531-9e7cd090e39a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "e7d41f01-cc9d-44ed-b531-9e7cd090e39a",
    "outputId": "2a23b640-1294-4623-af0b-7cacd178eeb8"
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "all_samples = []\n",
    "mean_prediction = []\n",
    "for i in range(0,N):\n",
    "    sample = mcs_sample_human_utility()\n",
    "    all_samples.append(sample)\n",
    "    mean_prediction.append(np.mean(all_samples))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(0,N),mean_prediction)\n",
    "plt.title(\"A plot showing the predicted mean of the human utility distribution\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1a2c5-0085-4d9b-9a80-a8b08799e0d6",
   "metadata": {
    "id": "f0a1a2c5-0085-4d9b-9a80-a8b08799e0d6"
   },
   "source": [
    "The distribution of these predictions can also be visualised by running multiple iterations of the sample process. This gives us a distribution of possible mean values. This is seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa5586-b521-40d4-b1ba-6fc2a45fb350",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "53aa5586-b521-40d4-b1ba-6fc2a45fb350",
    "outputId": "6aa91ce6-d5f8-4bea-9130-46d52283e626"
   },
   "outputs": [],
   "source": [
    "num_plots = 10\n",
    "for j in range(0, num_plots):\n",
    "    all_samples = []\n",
    "    mean_prediction = []\n",
    "    for i in range(0,N):\n",
    "        sample = mcs_sample_human_utility()\n",
    "        all_samples.append(sample)\n",
    "        mean_prediction.append(np.mean(all_samples))\n",
    "\n",
    "    plt.plot(range(0,N),mean_prediction)\n",
    "\n",
    "plt.title(\"A plot showing the distribution of predicted means for the human utility distribution\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZduLH1ldYroD",
   "metadata": {
    "id": "ZduLH1ldYroD"
   },
   "source": [
    "Using the plot above, we can conclude that the gamers willingness to use ammo is around 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7a00c-10b9-4a1b-aad2-8cac5478a451",
   "metadata": {
    "id": "ebd7a00c-10b9-4a1b-aad2-8cac5478a451"
   },
   "source": [
    "## 2. An Introduction to Markov Chain Monte Carlo (MCMC) <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "Using Monte Carlo estimation is great when we can sample from a distribution. However, there are times when we can't sample directly from a distribution. Lets say we are using Bayes rule to calculate a posterior distribution i.e.:\n",
    "\n",
    "$$ P(A | B) = \\frac{P (B | A) P(A)}{P(B)} = \\frac{P (B | A) P(A)}{\\int_{-\\infty}^{\\infty} P (B | A) P(A).dx} $$\n",
    "\n",
    "In many of the cases you have seen before, $P(A|B)$ might have been easy to sample from. For example, in the cases where $P(A|B)$ is in the form of a known distribution. However, in most complex problems, it is a complex function that doesn't match any known distributions. Markov Chain Monte Carlo gives us a way to sample from these distributions.\n",
    "\n",
    "#### Markov Chains\n",
    "\n",
    "A Markov chain is a sequence of random variables $X_1, X_2, X_3, ...$ which satisfy the markov property:\n",
    "\n",
    "For any $n \\in \\mathbb{N}$:\n",
    "\n",
    "$$P(X_n = i_n | X_{n-1} = i_{n-1}) = P(X_n = i_n | X_{0} = i_{0}, X_{1} = i_{1}, ...,  X_{n-1} = i_{n-1}) $$\n",
    "\n",
    "In other words, we only need to know the previous state in order to calculate the next state.\n",
    "\n",
    "\n",
    "The goal of MCMC methods is to generate a Markov chain where each random variable represents a sample from the target distribution.\n",
    "\n",
    "#### Running Example\n",
    "\n",
    "Using the human utility example, we know that the human's propensity to use ammo can be modelled by a $N(\\mu, 0.1)$ distribution. We ask an expert about what value $\\mu$ can take. They don't know exactly but narrow it down to a range between 2 and 6. Furthermore, we run some human trials and get the following values [3.1, 2.65]. We can calculate the likelihood by the following:\n",
    "\n",
    "$$P(data | \\mu, \\sigma) = \\prod_{i=0}^1 \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{-(x_i - \\mu)^2}{2 \\sigma^2}}$$\n",
    "\n",
    "$$P(data | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{-(3.1 - \\mu)^2}{2 \\sigma^2}} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{-(2.65 - \\mu)^2}{2 \\sigma^2}}$$\n",
    "\n",
    "$$P(data | \\mu, \\sigma) = \\frac{1}{2 \\pi \\sigma^2} e^{\\frac{-1}{2 \\pi \\sigma^2} \\left( (3.1 - \\mu)^2 + (2.65 - \\mu)^2 \\right)} $$\n",
    "\n",
    "We can combine this with a uniform(2,6) prior as follows:\n",
    "\n",
    "$$ P(data | \\mu) P (\\mu) = \\frac{1}{2 \\pi \\sigma^2} e^{\\frac{-1}{2 \\pi \\sigma^2} \\left( (3.1 - \\mu)^2 + (2.65 - \\mu)^2 \\right)} * \\frac{1}{6 - 2}$$\n",
    "\n",
    "$$ P(data | \\mu) P (\\mu) = \\frac{1}{0.08 \\pi} e^{\\frac{-1}{0.02 \\pi} \\left( (3.1 - \\mu)^2 + (2.65 - \\mu)^2 \\right)}$$\n",
    "\n",
    "where the function is 0 for all values outside the range [2,6].\n",
    "\n",
    "This can be visualised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59fa61-c414-4a8c-ab83-b9f6b78d24cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "3f59fa61-c414-4a8c-ab83-b9f6b78d24cc",
    "outputId": "56f9f32c-d6db-4cec-f0ee-1a62c592b79b"
   },
   "outputs": [],
   "source": [
    "xs = np.arange(1.5,4.5,0.01)\n",
    "ys = 1/(0.08 * np.pi) * np.exp(-1/(0.02 * np.pi) * ((3.1 - xs)**2 + (2.65 - xs)**2))\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JoquoweZZ8d-",
   "metadata": {
    "id": "JoquoweZZ8d-"
   },
   "source": [
    "By looking at the plot, we see that this follows a normal distribution with mean around 2.9. However, it's hard to know the exact parameters.\n",
    "\n",
    "In some cases, the distribution may follow some distribution which is entirely unrecognisable. This is where MCMC methods come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be6cea-1c4e-4a9d-88fe-4082cbbdceb5",
   "metadata": {
    "id": "d8be6cea-1c4e-4a9d-88fe-4082cbbdceb5"
   },
   "source": [
    "### 2.1. Metropolis\n",
    "\n",
    "In this section, we will explore the Metropolis algorithm, a Markov Chain Monte Carlo (MCMC) method used to sample from a probability distribution. For example, the distribution above. It does this by generating a Markov chain of samples $X_1, X_2, X_3,...$ from the distribution. The Metropolis algorithm is the original form of this class of algorithms and lays the foundations for more advanced algorithms, such as the Metropolis-Hastings algorithm.   \n",
    "\n",
    "#### Stationary Distribution\n",
    "\n",
    "The stationary distribution of a Markov Chain is reached when the probability distribution it represents remains unchanged over time. This means that if a Markov chain is in its stationary distribution, then after any number of transitions, the distribution of the states remains the same.This is represented mathematically as:\n",
    "\n",
    "$$\\pi(x) = \\sum_{x'} \\pi(x')P(x' \\rightarrow x)$$\n",
    "\n",
    "where $P$ is the transition probability. The equation can be interpreted as the probability of being in a state $x$ after one transition is the same as before the transition i.e. stationary.\n",
    "\n",
    "This seems like a good property for our Markov chain to have. Essentially it tells us that once one sample from the stationary distribution (target distribution) as been obtained, all other samples will belong to that distribution.\n",
    "\n",
    "#### Simple Markov Chain Example\n",
    "\n",
    "Let's start by simulating a simple Markov chain. The transition matrix $P$ below defines the probabilities of moving from one state to another. This matrix is 3x3, so it represents a state space with $3$ states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa7abc4-814c-4d3c-8b06-75f03557309a",
   "metadata": {
    "id": "6fa7abc4-814c-4d3c-8b06-75f03557309a"
   },
   "outputs": [],
   "source": [
    "# Define the transition matrix\n",
    "P = np.array([\n",
    "    [0.1, 0.6, 0.3],\n",
    "    [0.4, 0.4, 0.2],\n",
    "    [0.2, 0.3, 0.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LIQLC58IuuzF",
   "metadata": {
    "id": "LIQLC58IuuzF"
   },
   "source": [
    "To simulate the Markov chain for $n$ steps, we repeatedly use the proabilities from the $i^{\\text{th}}$ row of P to generate the distribution of transitions. The random.choice class picks the next state according to the distribution and the next state is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVnU5tAquycs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "mVnU5tAquycs",
    "outputId": "0cce42f1-c86c-4950-c54d-fa0db98503fb"
   },
   "outputs": [],
   "source": [
    "# Number of steps to simulate\n",
    "n_steps = 100\n",
    "\n",
    "# Initialize the state\n",
    "state = 0\n",
    "states = [state]\n",
    "\n",
    "# Simulate the Markov chain\n",
    "for _ in range(n_steps):\n",
    "    state = np.random.choice([0, 1, 2], p=P[state])\n",
    "    states.append(state)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(states, 'o-')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('State')\n",
    "plt.title('Markov Chain Simulation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M-Y_aLPrSSfz",
   "metadata": {
    "id": "M-Y_aLPrSSfz"
   },
   "source": [
    "One way to estimate the stationary distribution is to look at the proportion of time the Markov chain spends in each state. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65mL8J5rScyq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65mL8J5rScyq",
    "outputId": "47fb3e6b-fa7f-4211-c599-a1c99c10a744"
   },
   "outputs": [],
   "source": [
    "print(\"Proportion of zeros: {}\".format(states.count(0)/len(states)))\n",
    "print(\"Proportion of ones: {}\".format(states.count(1)/len(states)))\n",
    "print(\"Proportion of twos: {}\".format(states.count(2)/len(states)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wosHg3GgxQ3a",
   "metadata": {
    "id": "wosHg3GgxQ3a"
   },
   "source": [
    "You can also do this more acurately by solving the equation $\\pi P = \\pi$, where $\\pi$ is the stationary distribution vector, and $P$ is the transition matrix. Try this with pen and paper.\n",
    "\n",
    "This can also be done with code. Check your answer matches the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eQmfsvodxQge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQmfsvodxQge",
    "outputId": "638986bb-0ac0-44b0-c08b-44282b3286e1"
   },
   "outputs": [],
   "source": [
    "# Solve for the stationary distribution\n",
    "eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
    "stationary_dist = eigenvectors[:, np.isclose(eigenvalues, 1)]\n",
    "stationary_dist = stationary_dist[:, 0]\n",
    "stationary_dist = stationary_dist / stationary_dist.sum()  # Normalize\n",
    "print('Stationary Distribution:', stationary_dist.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vAMin_9Q-YrM",
   "metadata": {
    "id": "vAMin_9Q-YrM"
   },
   "source": [
    "This will compute the stationary distribution for the given transition matrix $P$. Try different transition matrices and observe how the stationary distribution changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y3_3YT3Q_qdY",
   "metadata": {
    "id": "Y3_3YT3Q_qdY"
   },
   "outputs": [],
   "source": [
    "# Try to explore some other transition matrices\n",
    "P_2 =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OP0a-a14ugjK",
   "metadata": {
    "id": "OP0a-a14ugjK"
   },
   "source": [
    "\n",
    "\n",
    "#### Detailed Balance\n",
    "\n",
    "The detailed balance condition is one way to ensure that the desired distribution is the stationary distribution of the Markov chain. In other words, it ensures that the above property holds. For a Markov chain with a transition probability matrix $P(x \\rightarrow x')$, the detailed balance condition requires that for every pair of states $x$ and $x'$, the following equation holds:\n",
    "\n",
    "$$\\pi(x)P(x \\rightarrow x') = \\pi(x')P(x' \\rightarrow x)$$\n",
    "\n",
    "Where $\\pi(x)$ is the stationary distribution of the Markov chain. If the detailed balance condition holds, then the Markov chain will eventually converge to the stationary distribution. The equation can be interpreted as the rate at which transitions from $x$ to $x'$ occur is the same as $x'$ to $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0XchmLhAEjZx",
   "metadata": {
    "id": "0XchmLhAEjZx"
   },
   "source": [
    "\n",
    "The Metropolis Algorithm Steps:\n",
    "  1. **Initialization:** Start with an initial state $x_0$\n",
    "  2. **Proposal Step:** Generate a proposed state $x^*$ from an arbitrary proposal distribution $q(x^* | x_t)$.\n",
    "  3. **Acceptance Step:** Compute the acceptance probability:\n",
    "  $$ \\alpha = \\min \\left( 1, \\frac{\\pi (x^*)}{\\pi(x_t)} \\right)$$\n",
    "  If accepted, move to the new state $x^*$; otherwise, stay in the current state x.\n",
    "  4. **Iteration:** Repeat the proposal and acceptance steps for the required number of iterations to generate the Markov chain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y5pgNFwjA9c8",
   "metadata": {
    "id": "Y5pgNFwjA9c8"
   },
   "source": [
    "\n",
    "Satisfying detailed balance means that the distribution is stationary, since:\n",
    "\n",
    "1. There is no net flow between states. i.e. over time, the transitions between states \"cancel out\" each other, preventing any long-term drift in the distribution of states.\n",
    "\n",
    "2. Since the inflow and outflow of probability between any two states are equal, the overall probability distribution does not change over time, meaning that the distribution remains stationary.\n",
    "\n",
    "\n",
    "#### Example code:\n",
    "\n",
    "First we create a function which represents the probability of data given $\\mu$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n-1eK5If1lJg",
   "metadata": {
    "id": "n-1eK5If1lJg"
   },
   "outputs": [],
   "source": [
    "# First, we define the unnormalized target distribution\n",
    "def target_distribution(mu):\n",
    "    # Constants as given in the equation\n",
    "    c1, c2 = 3.1, 2.65\n",
    "    constant = 1 / (0.08 * np.pi)\n",
    "    exponent = -1 / (0.02 * np.pi) * ((c1 - mu)**2 + (c2 - mu)**2)\n",
    "    return constant * np.exp(exponent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R3S1rTE819LO",
   "metadata": {
    "id": "R3S1rTE819LO"
   },
   "source": [
    "We now wish to find the probability of $\\mu$ given the data, using metropolis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cn2DQ3UW18CU",
   "metadata": {
    "id": "cn2DQ3UW18CU"
   },
   "outputs": [],
   "source": [
    "# Metropolis algorithm\n",
    "def metropolis_sampling(start_mu, n_samples, proposal_width):\n",
    "    samples = []\n",
    "    mu_current = start_mu\n",
    "    samples.append(mu_current)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Propose a new candidate mu from a normal distribution\n",
    "        mu_proposal = np.random.normal(mu_current, proposal_width)\n",
    "\n",
    "        # Compute acceptance probability\n",
    "        acceptance_ratio = target_distribution(mu_proposal) / target_distribution(mu_current)\n",
    "\n",
    "        # Accept or reject the proposal\n",
    "        if np.random.rand() < acceptance_ratio:\n",
    "            mu_current = mu_proposal  # Accept the proposal\n",
    "\n",
    "        samples.append(mu_current)\n",
    "\n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LJq3tHe62xLo",
   "metadata": {
    "id": "LJq3tHe62xLo"
   },
   "source": [
    "We can now run metropolis with an initial guess of 2 for 1,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pewiIsLJ2O08",
   "metadata": {
    "id": "pewiIsLJ2O08"
   },
   "outputs": [],
   "source": [
    "# Parameters for sampling\n",
    "start_mu = 1.5  # Initial guess for mu\n",
    "n_samples = 1000  # Number of samples to generate\n",
    "proposal_width = 0.2  # Width of the proposal distribution\n",
    "\n",
    "# Run the Metropolis sampler\n",
    "samples = metropolis_sampling(start_mu, n_samples, proposal_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XnEkPney3A4q",
   "metadata": {
    "id": "XnEkPney3A4q"
   },
   "source": [
    "We can plot a histogram of the predicted value for mu by plotting the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hfXq1j1D3AMA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "hfXq1j1D3AMA",
    "outputId": "20233db7-4f35-4baf-9bb5-9d04e2094cf0"
   },
   "outputs": [],
   "source": [
    "# Plot the sampled distribution\n",
    "plt.hist(samples, bins=60, density=True)\n",
    "plt.title('Sampled Distribution of $\\mu$')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Klx7W3zp3K4_",
   "metadata": {
    "id": "Klx7W3zp3K4_"
   },
   "source": [
    "Furthermore, we can observe the markov chain. Specifically, we look at the first 250 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lES5ZJOZ3Naa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "lES5ZJOZ3Naa",
    "outputId": "0e0313db-88fb-4f4d-d4e3-0ca3796414d0"
   },
   "outputs": [],
   "source": [
    "# Plot the markov chain\n",
    "plt.plot(samples[:250])\n",
    "plt.title('Sampled Markov chain of $\\mu$')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KoN2OR5-336K",
   "metadata": {
    "id": "KoN2OR5-336K"
   },
   "source": [
    "By looking at this plot, we can estimate when the Markov chain has converged to the stationary distribution. Notice how there is a trail of outliers in the first 15 or so steps!\n",
    "\n",
    "In some cases, the Markov chain may take longer to converge. This can have considerable effect on our prediction for the relevant parameter. To counteract this, we discard the first $n$ samples. This is a process known as setting the 'Burn In' value. Can you select an appropriate value for the burn in?\n",
    "\n",
    "Plot the markov chain and the parameter distribution below after considering an appropriate burn in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F2OWbhGa41HI",
   "metadata": {
    "id": "F2OWbhGa41HI"
   },
   "outputs": [],
   "source": [
    "# Plot your solution here:\n",
    "\n",
    "burn_in =\n",
    "\n",
    "# Plot the markov chain\n",
    "plt.plot(samples[:burn_in])\n",
    "plt.title('Sampled Markov chain of $\\mu$ with burn in')\n",
    "plt.xlabel('step - burn_in')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7404fc-2f83-4d37-8a3f-d878e4a1073d",
   "metadata": {
    "id": "3d7404fc-2f83-4d37-8a3f-d878e4a1073d"
   },
   "source": [
    "### 2.2. Metropolis-Hastings\n",
    "\n",
    "The Metropolis-Hastings algorithm is a generalization of the Metropolis algorithm, which was covered in the previous section. These methods are used to generate samples from complex probability distributions, particularly when direct sampling is challenging. The Metropolis-Hastings algorithm extends the original Metropolis algorithm by introducing an asymetric approach to the proposal distribution, making it applicable to a broader range of problems.\n",
    "\n",
    "The Metropolis-Hastings Algorithm Steps:\n",
    "  1. **Initialization:** Start with an initial state $x_0$\n",
    "  2. **Proposal Step:** Generate a proposed state $x^*$ from an arbitrary proposal distribution $q(x^* | x_t)$, which does not need to be symmetric.\n",
    "  3. **Acceptance Step:** Compute the acceptance probability:\n",
    "  $$ \\alpha = \\min \\left( 1, \\frac{\\pi (x^*) \\cdot q(x_t | x^*)}{\\pi(x_t) \\cdot q(x^* | x_t)} \\right)$$\n",
    "  \n",
    "  Here, the acceptance probability accounts for the asymmetry in the proposal distribution. Accept the proposed state $x^*$ with probability $\\alpha.$ If the proposal is rejected, the chain remains at the current state $x_t$.\n",
    "  4. **Iteration:** Repeat the proposal and acceptance steps for the required number of iterations to generate the Markov chain.\n",
    "\n",
    "#### Differences Between Metropolis and Metropolis-Hastings\n",
    "\n",
    "The key differences between the Metropolis and Metropolis-Hastings algorithms lie in the flexibility and generality of the proposal distribution:\n",
    "\n",
    "  1. **Asymetric Proposal Distribution:**\n",
    "\n",
    "  The Metropolis-Hastings algorithm allows the use of an asymmetric proposal distirbution. This means that the Metropolis algorithm is still a Metropolis-Hastings algorithm, since the asymmetry isn't strict.\n",
    "\n",
    "  2. **Acceptance Probability:**\n",
    "\n",
    "  In the Metropolis-Hastings algorithm, the acceptance probability $\\alpha$ contains aditional terms $q(x_t | x^*)$ and $q(x^* | x_t)$ to account for the asymmetry.\n",
    "\n",
    "  3. **Flexibility:**\n",
    "\n",
    "  The abiliity to use an asymmetric probosal distribution allows for more tailored exploration of the state space. This can potentially lead to faster convergence, particularly in higher-dimensional or complex distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vhS8F__26Teu",
   "metadata": {
    "id": "vhS8F__26Teu"
   },
   "source": [
    "#### Running Example\n",
    "\n",
    "Continuing with the same example as before, we modify the proposal distribution to be a log-normal. This is an asymmetric distribution, so Metropolis-Hastings must be used.\n",
    "\n",
    "First, we define the proposal distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UivyAkzu5tnf",
   "metadata": {
    "id": "UivyAkzu5tnf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Log-normal proposal distribution\n",
    "def proposal(mu_current, proposal_width):\n",
    "    # Propose a new candidate mu from a log-normal distribution (asymmetric)\n",
    "    return np.random.lognormal(mean=np.log(mu_current), sigma=proposal_width)\n",
    "\n",
    "# The probability density of the log-normal proposal\n",
    "def proposal_density(mu_new, mu_current, proposal_width):\n",
    "    # Log-normal PDF: f(x|μ,σ) = (1 / (xσ√2π)) exp(-(ln(x)−μ)^2 / 2σ^2)\n",
    "    if mu_new <= 0:  # Log-normal is undefined for non-positive values\n",
    "        return 0\n",
    "    return (1 / (mu_new * proposal_width * np.sqrt(2 * np.pi))) * \\\n",
    "           np.exp(- (np.log(mu_new) - np.log(mu_current))**2 / (2 * proposal_width**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SNiatpP06vOP",
   "metadata": {
    "id": "SNiatpP06vOP"
   },
   "source": [
    "Next, we define the Metropolis-Hastings algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QKBbx1th6yh9",
   "metadata": {
    "id": "QKBbx1th6yh9"
   },
   "outputs": [],
   "source": [
    "# Metropolis-Hastings algorithm\n",
    "def metropolis_hastings_sampling(start_mu, n_samples, proposal_width):\n",
    "    samples = []\n",
    "    mu_current = start_mu\n",
    "    samples.append(mu_current)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Propose a new candidate mu from the asymmetric proposal distribution\n",
    "        mu_proposal = proposal(mu_current, proposal_width)\n",
    "\n",
    "        # Calculate the acceptance ratio, adjusted by the proposal densities\n",
    "        acceptance_ratio = (target_distribution(mu_proposal) / target_distribution(mu_current)) * \\\n",
    "                           (proposal_density(mu_current, mu_proposal, proposal_width) / proposal_density(mu_proposal, mu_current, proposal_width))\n",
    "\n",
    "        # Accept or reject the proposal\n",
    "        if np.random.rand() < acceptance_ratio:\n",
    "            mu_current = mu_proposal  # Accept the proposal\n",
    "\n",
    "        samples.append(mu_current)\n",
    "\n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gtNoiaPO63ns",
   "metadata": {
    "id": "gtNoiaPO63ns"
   },
   "source": [
    "Notice how this is similar to the Metropolis algorithm but we modify the acceptance ratio to account for the asymmetry.\n",
    "\n",
    "Below, we run the algorithm and plot the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2qYp_ht7G78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "p2qYp_ht7G78",
    "outputId": "eba87763-fdb8-41d7-aa8f-804997101d20"
   },
   "outputs": [],
   "source": [
    "# Parameters for sampling\n",
    "start_mu = 1  # Initial guess for mu\n",
    "n_samples = 1000  # Number of samples to generate\n",
    "proposal_width = 0.025  # Width of the proposal distribution\n",
    "\n",
    "# Run the Metropolis-Hastings sampler\n",
    "samples = metropolis_hastings_sampling(start_mu, n_samples, proposal_width)\n",
    "\n",
    "# Plot the sampled distribution\n",
    "plt.hist(samples, bins=60, density=True)\n",
    "plt.title('Sampled Distribution of $\\mu$ (Asymmetric Metropolis-Hastings)')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3SYXzZL7XXA",
   "metadata": {
    "id": "O3SYXzZL7XXA"
   },
   "source": [
    "Again, we plot the Markov Chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q6ZmTPEe7cId",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "Q6ZmTPEe7cId",
    "outputId": "5445bfdc-26c6-4f54-9b9b-6c3647961c29"
   },
   "outputs": [],
   "source": [
    "# Plot the markov chain\n",
    "plt.plot(samples)\n",
    "plt.title('Sampled Markov chain of $\\mu$')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5r7isr7z7ioc",
   "metadata": {
    "id": "5r7isr7z7ioc"
   },
   "source": [
    "Try to find an appropriate burn in rate for this Markov Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vW8NOJEg7m7I",
   "metadata": {
    "id": "vW8NOJEg7m7I"
   },
   "outputs": [],
   "source": [
    "# Plot your solution here:\n",
    "\n",
    "burn_in =\n",
    "\n",
    "# Plot the markov chain\n",
    "plt.plot(samples[:burn_in])\n",
    "plt.title('Sampled Markov chain of $\\mu$ with burn in')\n",
    "plt.xlabel('step - burn_in')\n",
    "plt.ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t_jupBeJ76tL",
   "metadata": {
    "id": "t_jupBeJ76tL"
   },
   "source": [
    "Experiment with the parameters of this algorithm. Notice what happens when you change the start_mu or proposal_width."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b834fea-6ad4-4f43-8052-4f6e512f06c7",
   "metadata": {
    "id": "5b834fea-6ad4-4f43-8052-4f6e512f06c7"
   },
   "source": [
    "### 2.3. Gibbs Sampling\n",
    "\n",
    "Gibbs Sampling is a Markov Chain Monte Carlo (MCMC) method used for generating samples from a joint probability distribution when direct sampling is difficult. It is particularly effective when dealing with high-dimensional distributions. Instead of sampling from the full joint distribution, Gibbs Sampling samples each variable in turn, conditioned on the current values of the other variables. Over many iterations, this process produces samples that approximate the true joint distribution. This decomposition allows for efficient sampling in complex models, especially in Bayesian networks and hierarchical models.\n",
    "\n",
    "#### The Gibbs Sampling Process\n",
    "\n",
    "1. **Initialization**: Start with an initial value for each variable.\n",
    "2. **Sampling**: For each variable, sample from its conditional distribution given the current values of all other variables.\n",
    "3. **Iteration**: Repeat the process for a large number of iterations to approximate the joint distribution.\n",
    "\n",
    "\n",
    "#### Simple Example: Binary Variables\n",
    "\n",
    "Consider a simple model with two binary variables $X_1$ and $X_2$, where each variable can take the value 0 or 1. Suppose we know the following conditional distributions:\n",
    "\n",
    "- $ P(X_1 | X_2 = 0) = 0.3$ and $P(X_1 | X_2 = 1) = 0.8$\n",
    "- $P(X_2 | X_1 = 0) = 0.4 $ and $P(X_2 | X_1 = 1) = 0.7$\n",
    "\n",
    "We want to sample from the joint distribution \\( P(X_1, X_2) \\) using Gibbs Sampling.\n",
    "\n",
    "\n",
    "#### Gibbs Sampling Implementation\n",
    "\n",
    "Here's how you might implement Gibbs Sampling for this simple two-variable case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mekQgFpzCykq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mekQgFpzCykq",
    "outputId": "63f99419-c699-4fb4-f161-cf914c016e89"
   },
   "outputs": [],
   "source": [
    "# Define the conditional probabilities\n",
    "def sample_x1_given_x2(x2):\n",
    "    return np.random.rand() < (0.8 if x2 == 1 else 0.3)  # Returns true or false\n",
    "\n",
    "def sample_x2_given_x1(x1):\n",
    "    return np.random.rand() < (0.7 if x1 == 1 else 0.4)  # Returns true or false\n",
    "\n",
    "# Initialize the variables\n",
    "x1, x2 = 0, 0\n",
    "\n",
    "# Perform Gibbs sampling\n",
    "n_samples = 10000\n",
    "samples = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    # Sample X1 given X2\n",
    "    x1 = 1 if sample_x1_given_x2(x2) else 0\n",
    "    # Sample X2 given X1\n",
    "    x2 = 1 if sample_x2_given_x1(x1) else 0\n",
    "\n",
    "    samples.append((x1, x2))\n",
    "\n",
    "# Convert samples to a numpy array for analysis\n",
    "samples = np.array(samples)\n",
    "\n",
    "# Calculate the empirical joint distribution\n",
    "joint_distribution = np.zeros((2, 2))\n",
    "for sample in samples:\n",
    "    joint_distribution[sample[0], sample[1]] += 1\n",
    "joint_distribution /= n_samples\n",
    "\n",
    "print(\"Empirical joint distribution P(X1, X2):\")\n",
    "print(joint_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TuFkOpYREL-G",
   "metadata": {
    "id": "TuFkOpYREL-G"
   },
   "source": [
    "Visualising results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OaxMxKrID_OV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "OaxMxKrID_OV",
    "outputId": "850c6bb7-9351-41df-a279-09598a46dbcf"
   },
   "outputs": [],
   "source": [
    "# Plotting the empirical joint distribution\n",
    "sns.heatmap(joint_distribution, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=[\"X2=0\", \"X2=1\"], yticklabels=[\"X1=0\", \"X1=1\"])\n",
    "plt.title(\"Empirical Joint Distribution P(X1, X2)\")\n",
    "plt.xlabel(\"X2\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ZPyWJwK8d6D",
   "metadata": {
    "id": "7ZPyWJwK8d6D"
   },
   "source": [
    "Throughout the rest of this section, we will apply Gibbs sampling to the running example. Consider that we also don't know the value of the standard deviation $\\sigma$ and wish to find the joint distribution $p(\\mu, \\sigma | data)$, when\n",
    "$$p(\\mu, \\sigma | data) ∝ P(data | \\mu) P (\\mu) P(\\sigma)$$\n",
    "\n",
    "$$ P(data | \\mu) P (\\mu) ∝ \\frac{1}{8 \\pi \\sigma^2} e^{\\frac{-1}{2 \\pi \\sigma^2} \\left( (3.1 - \\mu)^2 + (2.65 - \\mu)^2 \\right)}$$\n",
    "\n",
    "We must first specify a prior for $\\sigma$. A common one is the Inverse-Gamma distribution, defined as follows:\n",
    "$$\\text{Inverse-Gamma}(x \\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{-(\\alpha+1)} e^{-\\frac{\\beta}{x}}, \\quad x > 0, \\, \\alpha > 0, \\, \\beta > 0$$\n",
    "\n",
    "We can plot this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-yg8oh6CCLrM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "-yg8oh6CCLrM",
    "outputId": "d65c765a-83ab-4324-fa09-86352934f0a9"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import invgamma\n",
    "# Parameters for the Inverse-Gamma distribution\n",
    "alpha = 2  # Shape parameter\n",
    "beta = 1   # Scale parameter\n",
    "\n",
    "# Generate x values for plotting\n",
    "x = np.linspace(0.01, 10, 1000)\n",
    "\n",
    "# Compute the Inverse-Gamma PDF\n",
    "pdf = invgamma.pdf(x, alpha, scale=beta)\n",
    "\n",
    "# Plot the Inverse-Gamma distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, pdf, label=f'Inverse-Gamma PDF (α={alpha}, β={beta})', color='blue')\n",
    "plt.title('Inverse-Gamma Distribution')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xIHnGZhSCiMD",
   "metadata": {
    "id": "xIHnGZhSCiMD"
   },
   "source": [
    "Now we can write the joint posterior:\n",
    "\n",
    "$$p(\\mu,\\sigma^2 \\mid data)∝\\sigma^2 e^ \\frac{-1}{2 \\sigma^2}((3.1-\\mu)^2+(2.65-\\mu)^2)\\cdot \\text{Inverse-Gamma}(\\sigma^2\\mid \\alpha,\\beta)$$\n",
    "\n",
    "and estimate it using Gibbs sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3M2WVD0d8dlx",
   "metadata": {
    "id": "3M2WVD0d8dlx"
   },
   "outputs": [],
   "source": [
    "# known data points\n",
    "data1 = 3.1\n",
    "data2 = 2.65\n",
    "\n",
    "# Gibbs Sampling\n",
    "def gibbs_sampling(n_samples, start_mu, start_sigma2):\n",
    "    samples_mu = []\n",
    "    samples_sigma2 = []\n",
    "\n",
    "    # Initial values\n",
    "    mu_current = start_mu\n",
    "    sigma2_current = start_sigma2\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # 1. Sample mu | sigma^2, data\n",
    "        mu_mean = (data1 + data2) / 2\n",
    "        mu_variance = sigma2_current / 2\n",
    "        mu_current = np.random.normal(mu_mean, np.sqrt(mu_variance))\n",
    "\n",
    "        # 2. Sample sigma^2 | mu, data\n",
    "        shape = alpha + 1  # Inverse-Gamma shape parameter\n",
    "        scale = beta + ((data1 - mu_current)**2 + (data2 - mu_current)**2) / 2  # Inverse-Gamma scale parameter\n",
    "        sigma2_current = invgamma.rvs(a=shape, scale=scale)\n",
    "\n",
    "        # Store the samples\n",
    "        samples_mu.append(mu_current)\n",
    "        samples_sigma2.append(sigma2_current)\n",
    "\n",
    "    return np.array(samples_mu), np.array(samples_sigma2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rj1Vft4RA4cW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "Rj1Vft4RA4cW",
    "outputId": "33a78f03-b50f-408e-92b7-be8284429fa2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters for Gibbs sampling\n",
    "n_samples = 10000\n",
    "start_mu = 2.0  # Initial guess for mu\n",
    "start_sigma2 = 1.0  # Initial guess for sigma^2\n",
    "\n",
    "# Run Gibbs sampling\n",
    "samples_mu, samples_sigma2 = gibbs_sampling(n_samples, start_mu, start_sigma2)\n",
    "\n",
    "# Plot the sampled distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(samples_mu, bins=50, density=True)\n",
    "plt.title('Sampled Distribution of $\\mu$')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(samples_sigma2, bins=50, density=True)\n",
    "plt.title('Sampled Distribution of $\\sigma^2$')\n",
    "plt.xlabel('$\\sigma^2$')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zz-xNq2hDwR1",
   "metadata": {
    "id": "zz-xNq2hDwR1"
   },
   "source": [
    "We can also plot the samples similarly to before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D31gj-PcBLr3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "D31gj-PcBLr3",
    "outputId": "713da4bc-d9e6-4141-8bf3-774628dc379e"
   },
   "outputs": [],
   "source": [
    "# Scatter plot of the samples\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(samples_mu, samples_sigma2, alpha=0.2, s=10, color='blue')\n",
    "plt.title('Scatter Plot of $\\mu$ and $\\sigma^2$ Samples from Gibbs Sampler')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('$\\sigma^2$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAD-t0S8EOri",
   "metadata": {
    "id": "FAD-t0S8EOri"
   },
   "source": [
    "Looking along the $x$ axis, you can see that the distribution of $\\mu$ is similar to Metropolis and Metropolis-Hastings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9368f1de-fb84-408a-9611-9f8d2fcc4f9a",
   "metadata": {
    "id": "9368f1de-fb84-408a-9611-9f8d2fcc4f9a"
   },
   "source": [
    "### 2.4. Hamiltonian Monte Carlo\n",
    "\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) is a more advanced Markov Chain Monte Carlo (MCMC) method used to sample from complex probability distributions, especially in high-dimensional spaces. Unlike the previous methods (Gibbs Sampling), HMC uses Hamiltonian dynamics from physics to explore the target distribution more efficiently. HMC is particularly powerful in Bayesian statistics, where it is used to draw samples from posterior distributions that are difficult to sample from directly.\n",
    "\n",
    "\n",
    "HMC treats the sampling problem as a physical system where:\n",
    "- **Position Variables**: Represent the parameters of interest (e.g., the parameters of a Bayesian model).\n",
    "- **Momentum Variables**: Represent the way to explore the parameter space.\n",
    "\n",
    "The core idea of HMC is to simulate the dynamics of a particle moving through the parameter space under the influence of a potential energy defined by the negative log probability of the target distribution.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "- **Potential Energy $U(\\theta)$**: This is related to the target distribution $p(\\theta)$ by $U(\\theta) = -\\log p(\\theta)$. It represents the \"energy landscape\" that the particle moves through.\n",
    "- **Kinetic Energy $K(p)$**: Typically defined as $K(p) = \\frac{1}{2} p^T M^{-1} p$, where $p$ is the momentum vector and $M$ is the mass matrix (often taken as the identity matrix). This is analogous to the kinetic energy of a physical system.\n",
    "- **Hamiltonian $H(\\theta, p)$**: The total energy of the system, defined as $H(\\theta, p) = U(\\theta) + K(p)$. The goal is to preserve the Hamiltonian as the particle moves through the space, which ensures that the sampling respects the target distribution.\n",
    "\n",
    "We first define the log posterior and its differential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lFpXxA9dGEwt",
   "metadata": {
    "id": "lFpXxA9dGEwt"
   },
   "outputs": [],
   "source": [
    "# Log-posterior\n",
    "def log_posterior(mu, sigma, data):\n",
    "    if not (mu_prior_min <= mu <= mu_prior_max) or sigma <= 0:\n",
    "        return -np.inf\n",
    "    likelihood = -0.5 * np.sum((data - mu) ** 2) / (sigma ** 2) - len(data) * np.log(sigma)\n",
    "    prior_mu = 0  # Uniform(2, 6) has a flat log prior\n",
    "    prior_sigma = stats.invgamma.logpdf(sigma**2, alpha, scale=beta)\n",
    "    return likelihood + prior_mu + prior_sigma\n",
    "\n",
    "# Gradient of the log-posterior\n",
    "def grad_log_posterior(mu, sigma, data):\n",
    "    if not (mu_prior_min <= mu <= mu_prior_max) or sigma <= 0:\n",
    "        return np.array([-np.inf, -np.inf])\n",
    "\n",
    "    # Derivatives w.r.t. mu and sigma\n",
    "    grad_mu = np.sum(data - mu) / (sigma ** 2)\n",
    "    grad_sigma = -len(data) / sigma + np.sum((data - mu) ** 2) / (sigma ** 3)\n",
    "    # Gradient of the Inverse-Gamma prior for sigma\n",
    "    grad_sigma_prior = -(2 * alpha + 1) / sigma + 2 * beta / (sigma ** 3)\n",
    "\n",
    "    return np.array([grad_mu, grad_sigma + grad_sigma_prior])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NLMLOtmbVWfG",
   "metadata": {
    "id": "NLMLOtmbVWfG"
   },
   "source": [
    "Next, we set up the problem, with the prior parameters as well as the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y4cDppipVg8o",
   "metadata": {
    "id": "Y4cDppipVg8o"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats  # Used for inverse gamma\n",
    "\n",
    "# Priors\n",
    "mu_prior_min = 2\n",
    "mu_prior_max = 6\n",
    "alpha = 2  # Inverse-Gamma alpha\n",
    "beta = 1   # Inverse-Gamma beta\n",
    "\n",
    "# Data points\n",
    "data = np.array([3.1, 2.65])\n",
    "\n",
    "# Number of iterations for HMC\n",
    "n_iter = 5000\n",
    "L = 10  # Number of leapfrog steps\n",
    "epsilon = 0.01  # Step size for leapfrog\n",
    "\n",
    "# Initialize parameters\n",
    "mu = 4\n",
    "sigma = 1\n",
    "\n",
    "# Store samples\n",
    "mu_samples = np.zeros(n_iter)\n",
    "sigma_samples = np.zeros(n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U0bY4mk5Ghwa",
   "metadata": {
    "id": "U0bY4mk5Ghwa"
   },
   "source": [
    "<b>The Leapfrog method</b>\n",
    "\n",
    "Leapfrog is a numerical method used to simulate the motion of particles in a physical system governed by Newton's laws of motion. It is commonly used in both physics simulations and Hamiltonian Monte Carlo.\n",
    "\n",
    "The leapfrog method gets its name due to the updates for the position and momentum variables \"leaping\" over each other. This is similar to HMC, where the algorithm alterates between variables. The key advantage of the leapfrog method is that it conserves energy (approximately) and is reversible, making it well-suited for Hamiltonian dynamics.\n",
    "\n",
    "Here's a breakdown of the leapfrog algorithm:\n",
    "\n",
    "\n",
    "\n",
    "1. **Initial State:** Start with the initial values for the position $q$ and momentum $p$.\n",
    "2. **Half-step for Momentum:** Update the momentum $p$ by half a time step $\\frac{\\epsilon}{2}$, using the gradient of the potential energy with respect to the position:\n",
    "\n",
    "$$p ⟵ p - \\frac{\\epsilon}{2} ∇ V(q)$$\n",
    "\n",
    "3. **Full-step for Position:** Update the position $q$ by a full time step $\\epsilon$, using the updated momentum:\n",
    "\n",
    "$$ q ← q + \\epsilon p$$\n",
    "\n",
    "4. **Another Half-step for Momentum:** Finally, update the momentum $p$ again by another half time step $\\frac{\\epsilon}{2}$:\n",
    "\n",
    "$$p ⟵ p - \\frac{\\epsilon}{2} ∇ V(q)$$\n",
    "\n",
    "By combining these steps, the leapfrog method provides an efficient and stable way to simulate the dynamics of the system over time. The python code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QBT36SV7UZzi",
   "metadata": {
    "id": "QBT36SV7UZzi"
   },
   "outputs": [],
   "source": [
    "# Leapfrog method\n",
    "def leapfrog(mu, sigma, p_mu, p_sigma, epsilon, L, data):\n",
    "    mu_new = mu\n",
    "    sigma_new = sigma\n",
    "    p_mu_new = p_mu - 0.5 * epsilon * grad_log_posterior(mu, sigma, data)[0]\n",
    "    p_sigma_new = p_sigma - 0.5 * epsilon * grad_log_posterior(mu, sigma, data)[1]\n",
    "\n",
    "    for _ in range(L):\n",
    "        mu_new += epsilon * p_mu_new\n",
    "        sigma_new += epsilon * p_sigma_new\n",
    "        if _ != L - 1:\n",
    "            p_mu_new -= epsilon * grad_log_posterior(mu_new, sigma_new, data)[0]\n",
    "            p_sigma_new -= epsilon * grad_log_posterior(mu_new, sigma_new, data)[1]\n",
    "\n",
    "    p_mu_new -= 0.5 * epsilon * grad_log_posterior(mu_new, sigma_new, data)[0]\n",
    "    p_sigma_new -= 0.5 * epsilon * grad_log_posterior(mu_new, sigma_new, data)[1]\n",
    "\n",
    "    return mu_new, sigma_new, p_mu_new, p_sigma_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mTqHYiFdVw2p",
   "metadata": {
    "id": "mTqHYiFdVw2p"
   },
   "source": [
    "Using the Leapfrog algorithm, the Hamiltonian Monte Carlo algorithm can then be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "udJ2zgaGGdxh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "udJ2zgaGGdxh",
    "outputId": "dd4bbfb6-2cd0-4fb8-b4dc-a2b94971660c"
   },
   "outputs": [],
   "source": [
    "for i in range(n_iter):\n",
    "    # Sample initial momentum\n",
    "    p_mu = np.random.randn()\n",
    "    p_sigma = np.random.randn()\n",
    "\n",
    "    # Compute Hamiltonian at current state\n",
    "    current_log_posterior = log_posterior(mu, sigma, data)\n",
    "    current_H = -current_log_posterior + 0.5 * (p_mu ** 2 + p_sigma ** 2)\n",
    "\n",
    "    # Propose new state using leapfrog\n",
    "    mu_new, sigma_new, p_mu_new, p_sigma_new = leapfrog(mu, sigma, p_mu, p_sigma, epsilon, L, data)\n",
    "\n",
    "    # Compute Hamiltonian at new state\n",
    "    new_log_posterior = log_posterior(mu_new, sigma_new, data)\n",
    "    new_H = -new_log_posterior + 0.5 * (p_mu_new ** 2 + p_sigma_new ** 2)\n",
    "\n",
    "    # Accept or reject the new state based on Metropolis criterion\n",
    "    if np.random.rand() < np.exp(current_H - new_H):\n",
    "        mu, sigma = mu_new, sigma_new\n",
    "\n",
    "    # Store the samples\n",
    "    mu_samples[i] = mu\n",
    "    sigma_samples[i] = sigma\n",
    "\n",
    "\n",
    "\n",
    "# Results\n",
    "print(f\"Mean of mu samples: {np.mean(mu_samples)}\")\n",
    "print(f\"Mean of sigma samples: {np.mean(sigma_samples)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Scatter plot of the HMC samples\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(mu_samples, sigma_samples, alpha=0.2, s=10, color='green')\n",
    "plt.title('Scatter Plot of $\\mu$ and $\\sigma$ Samples from HMC')\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel('$\\sigma$')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eu0FS-TGGDYx",
   "metadata": {
    "id": "eu0FS-TGGDYx"
   },
   "source": [
    "In this example, we used Hamiltonian Monte Carlo to sample from a bivariate Gaussian distribution. The Hamiltonian dynamics allow us to efficiently explore the parameter space, even when the distribution is complex. The Leapfrog method is used to simulate the dynamics, ensuring that the sampler preserves the total energy of the system, which helps in generating accurate samples.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
